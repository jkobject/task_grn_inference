{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import anndata as ad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assemble the results from differnet runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r resources/results/all_main/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied identical file: method_configs.yaml\n",
      "Copied identical file: metric_configs.yaml\n",
      "Merged: dataset_uns.yaml\n",
      "Merged: score_uns.yaml\n",
      "Merged: trace.txt (duplicates removed)\n",
      "Copied unique file: state.yaml → state.yaml\n",
      "Copied unique file: op_.celloracle.celloracle.prediction.h5ad → op_.celloracle.celloracle.prediction.h5ad\n",
      "Copied unique file: scplus_mdata.h5mu.2DAaAA8E → scplus_mdata.h5mu.2DAaAA8E\n",
      "Copied unique file: op_.negative_control.negative_control.prediction.h5ad → op_.negative_control.negative_control.prediction.h5ad\n",
      "Copied unique file: op_.scenicplus.scenicplus.prediction.h5ad → op_.scenicplus.scenicplus.prediction.h5ad\n",
      "Copied unique file: op_.portia.portia.prediction.h5ad → op_.portia.portia.prediction.h5ad\n",
      "Copied unique file: op_.granie.granie.prediction.h5ad → op_.granie.granie.prediction.h5ad\n",
      "Copied unique file: op_.scprint.scprint.prediction.h5ad → op_.scprint.scprint.prediction.h5ad\n",
      "Copied unique file: op_.pearson_corr.pearson_corr.prediction.h5ad → op_.pearson_corr.pearson_corr.prediction.h5ad\n",
      "Copied unique directory: output → output\n",
      "Copied unique file: op_.scglue.scglue.prediction.h5ad → op_.scglue.scglue.prediction.h5ad\n",
      "Copied unique file: op_.ppcor.ppcor.prediction.h5ad → op_.ppcor.ppcor.prediction.h5ad\n",
      "Copied unique file: op_.positive_control.positive_control.prediction.h5ad → op_.positive_control.positive_control.prediction.h5ad\n",
      "Copied unique file: op_.scenic.scenic.prediction.h5ad → op_.scenic.scenic.prediction.h5ad\n",
      "Copied unique file: op_.figr.figr.prediction.h5ad → op_.figr.figr.prediction.h5ad\n",
      "Copied unique file: task_info.yaml → task_info.yaml\n",
      "Copied unique file: op_.grnboost2.grnboost2.prediction.h5ad → op_.grnboost2.grnboost2.prediction.h5ad\n",
      "Copied unique file: state.yaml → state_nakatake.yaml\n",
      "Copied unique file: nakatake_.positive_control.positive_control.prediction.h5ad → nakatake_.positive_control.positive_control.prediction.h5ad\n",
      "Copied unique file: nakatake_.portia.portia.prediction.h5ad → nakatake_.portia.portia.prediction.h5ad\n",
      "Copied unique file: nakatake_.grnboost.grnboost.prediction.h5ad → nakatake_.grnboost.grnboost.prediction.h5ad\n",
      "Copied unique file: nakatake_.ppcor.ppcor.prediction.h5ad → nakatake_.ppcor.ppcor.prediction.h5ad\n",
      "Copied unique file: nakatake_.pearson_corr.pearson_corr.prediction.h5ad → nakatake_.pearson_corr.pearson_corr.prediction.h5ad\n",
      "Copied unique file: nakatake_.negative_control.negative_control.prediction.h5ad → nakatake_.negative_control.negative_control.prediction.h5ad\n",
      "Copied unique file: nakatake_.scenic.scenic.prediction.h5ad → nakatake_.scenic.scenic.prediction.h5ad\n",
      "Copied unique file: task_info.yaml → task_info_nakatake.yaml\n",
      "Copied unique file: norman_.grnboost2.grnboost2.prediction.h5ad → norman_.grnboost2.grnboost2.prediction.h5ad\n",
      "Copied unique file: state.yaml → state_norman.yaml\n",
      "Copied unique file: norman_.ppcor.ppcor.prediction.h5ad → norman_.ppcor.ppcor.prediction.h5ad\n",
      "Copied unique file: norman_.scprint.scprint.prediction.h5ad → norman_.scprint.scprint.prediction.h5ad\n",
      "Copied unique file: norman_.pearson_corr.pearson_corr.prediction.h5ad → norman_.pearson_corr.pearson_corr.prediction.h5ad\n",
      "Copied unique file: norman_.portia.portia.prediction.h5ad → norman_.portia.portia.prediction.h5ad\n",
      "Copied unique file: norman_.negative_control.negative_control.prediction.h5ad → norman_.negative_control.negative_control.prediction.h5ad\n",
      "Copied unique file: norman_.scenic.scenic.prediction.h5ad → norman_.scenic.scenic.prediction.h5ad\n",
      "Copied unique file: norman_.positive_control.positive_control.prediction.h5ad → norman_.positive_control.positive_control.prediction.h5ad\n",
      "Copied unique file: task_info.yaml → task_info_norman.yaml\n",
      "Copied unique file: state.yaml → state_replogle.yaml\n",
      "Copied unique file: replogle_.pearson_corr.pearson_corr.prediction.h5ad → replogle_.pearson_corr.pearson_corr.prediction.h5ad\n",
      "Copied unique file: replogle_.positive_control.positive_control.prediction.h5ad → replogle_.positive_control.positive_control.prediction.h5ad\n",
      "Copied unique file: replogle_.portia.portia.prediction.h5ad → replogle_.portia.portia.prediction.h5ad\n",
      "Copied unique file: replogle_.grnboost2.grnboost2.prediction.h5ad → replogle_.grnboost2.grnboost2.prediction.h5ad\n",
      "Copied unique file: replogle_.negative_control.negative_control.prediction.h5ad → replogle_.negative_control.negative_control.prediction.h5ad\n",
      "Copied unique file: replogle_.ppcor.ppcor.prediction.h5ad → replogle_.ppcor.ppcor.prediction.h5ad\n",
      "Copied unique file: task_info.yaml → task_info_replogle.yaml\n",
      "Copied unique file: replogle_.scenic.scenic.prediction.h5ad → replogle_.scenic.scenic.prediction.h5ad\n",
      "Copied unique file: state.yaml → state_adamson.yaml\n",
      "Copied unique file: adamson_.portia.portia.prediction.h5ad → adamson_.portia.portia.prediction.h5ad\n",
      "Copied unique file: adamson_.negative_control.negative_control.prediction.h5ad → adamson_.negative_control.negative_control.prediction.h5ad\n",
      "Copied unique file: adamson_.pearson_corr.pearson_corr.prediction.h5ad → adamson_.pearson_corr.pearson_corr.prediction.h5ad\n",
      "Copied unique file: adamson_.ppcor.ppcor.prediction.h5ad → adamson_.ppcor.ppcor.prediction.h5ad\n",
      "Copied unique file: adamson_.scenic.scenic.prediction.h5ad → adamson_.scenic.scenic.prediction.h5ad\n",
      "Copied unique file: adamson_.grnboost2.grnboost2.prediction.h5ad → adamson_.grnboost2.grnboost2.prediction.h5ad\n",
      "Copied unique file: adamson_.positive_control.positive_control.prediction.h5ad → adamson_.positive_control.positive_control.prediction.h5ad\n",
      "Copied unique file: task_info.yaml → task_info_adamson.yaml\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "\n",
    "base_dir = 'resources/results/'\n",
    "save_dir = 'resources/results/all_main/'\n",
    "runs = ['op', 'nakatake', 'norman', 'replogle', 'adamson']\n",
    "\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# 1. Copy one version of the identical files\n",
    "identical_files = ['method_configs.yaml', 'metric_configs.yaml']\n",
    "for fname in identical_files:\n",
    "    src = os.path.join(base_dir, f'{runs[0]}_run', fname)\n",
    "    dst = os.path.join(save_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    print(f\"Copied identical file: {fname}\")\n",
    "\n",
    "# 2. Merge dataset_uns.yaml by appending all contents\n",
    "merged_uns = []\n",
    "for run in runs:\n",
    "    path = os.path.join(base_dir, f'{run}_run', 'dataset_uns.yaml')\n",
    "\n",
    "    with open(path, 'r') as f:\n",
    "        data = yaml.safe_load(f)\n",
    "        merged_uns.extend(data)\n",
    "        \n",
    "with open(os.path.join(save_dir, 'dataset_uns.yaml'), 'w') as f:\n",
    "    yaml.dump(merged_uns, f)\n",
    "print(\"Merged: dataset_uns.yaml\")\n",
    "\n",
    "# 3. Merge score_uns.yaml similarly\n",
    "merged_scores = []\n",
    "for run in runs:\n",
    "    path = os.path.join(base_dir, f'{run}_run', 'score_uns.yaml')\n",
    "\n",
    "    with open(path, 'r') as f:\n",
    "        data = yaml.safe_load(f)\n",
    "        # - remove those with missing (because of the metric)\n",
    "        data = [d for d in data if d is not None and 'missing' not in str(d)]\n",
    "        \n",
    "        # print(str(data[0]))\n",
    "        # aa\n",
    "        # missing\n",
    "        if data:\n",
    "            if isinstance(data, dict):\n",
    "                merged_scores.append(data)\n",
    "            elif isinstance(data, list):\n",
    "                merged_scores.extend(data)\n",
    "            else:\n",
    "                print(f\"Unexpected format in {path}: {type(data)}\")\n",
    "\n",
    "with open(os.path.join(save_dir, 'score_uns.yaml'), 'w') as f:\n",
    "    yaml.dump(merged_scores, f)\n",
    "print(\"Merged: score_uns.yaml\")\n",
    "\n",
    "# 4. Merge trace.txt with deduplication\n",
    "seen_lines = OrderedDict()\n",
    "for run in runs:\n",
    "    path = os.path.join(base_dir, f'{run}_run', 'trace.txt')\n",
    "    \n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            seen_lines[line] = None\n",
    "\n",
    "with open(os.path.join(save_dir, 'trace.txt'), 'w') as f:\n",
    "    for line in seen_lines.keys():\n",
    "        f.write(line)\n",
    "df = pd.read_csv(os.path.join(save_dir, 'trace.txt'), sep='\\t')\n",
    "df = df.drop_duplicates(subset=['name'])\n",
    "df.to_csv(os.path.join(save_dir, 'trace.txt'), sep='\\t')\n",
    "print(\"Merged: trace.txt (duplicates removed)\")\n",
    "\n",
    "# 5. Copy other unknown files/directories\n",
    "all_known = set(identical_files + ['dataset_uns.yaml', 'score_uns.yaml', 'trace.txt'])\n",
    "\n",
    "for run in runs:\n",
    "    run_dir = Path(base_dir) / f'{run}_run'\n",
    "    for file_path in run_dir.iterdir():\n",
    "        if file_path.name in all_known:\n",
    "            continue\n",
    "\n",
    "        dest_path = Path(save_dir) / file_path.name\n",
    "\n",
    "        if dest_path.exists():\n",
    "            dest_path = Path(save_dir) / f\"{file_path.stem}_{run}{file_path.suffix}\"\n",
    "\n",
    "        if file_path.is_file():\n",
    "            shutil.copyfile(file_path, dest_path)\n",
    "            print(f\"Copied unique file: {file_path.name} → {dest_path.name}\")\n",
    "        elif file_path.is_dir():\n",
    "            shutil.copytree(file_path, dest_path)\n",
    "            print(f\"Copied unique directory: {file_path.name} → {dest_path.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rename grnboost2 to grnboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated: resources/results/all_main/trace.txt\n",
      "Updated: resources/results/all_main/method_configs.yaml\n",
      "Updated: resources/results/all_main/score_uns.yaml\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "target_dir = 'resources/results/all_main'\n",
    "\n",
    "for root, _, files in os.walk(target_dir):\n",
    "    for fname in files:\n",
    "        fpath = os.path.join(root, fname)\n",
    "\n",
    "        # Skip binary files (optional safety)\n",
    "        try:\n",
    "            with open(fpath, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "        except UnicodeDecodeError:\n",
    "            continue  # likely a binary file, skip it\n",
    "\n",
    "        if 'grnboost2' in content:\n",
    "            content = content.replace('grnboost2', 'grnboost')\n",
    "            with open(fpath, 'w', encoding='utf-8') as f:\n",
    "                f.write(content)\n",
    "            print(f\"Updated: {fpath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Nakatake test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = ad.read_h5ad('resources/grn_benchmark/inference_data/nakatake_rna.h5ad')\n",
    "# Sample 100 cells\n",
    "sampled_cells = adata.obs.sample(n=100, random_state=0).index\n",
    "\n",
    "# Sample 2000 genes\n",
    "sampled_genes = adata.var.sample(n=2000, random_state=0).index\n",
    "\n",
    "# Subset the AnnData object\n",
    "adata_subset = adata[sampled_cells, sampled_genes].copy()\n",
    "adata_subset.write('resources_test/grn_benchmark/inference_data/nakatake_rna.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = ad.read_h5ad('resources/grn_benchmark/evaluation_data/nakatake_bulk.h5ad')\n",
    "# Sample 100 cells\n",
    "sampled_cells = adata.obs.sample(n=100, random_state=0).index\n",
    "\n",
    "# Sample 2000 genes\n",
    "sampled_genes = adata.var.sample(n=2000, random_state=0).index\n",
    "\n",
    "# Subset the AnnData object\n",
    "adata_subset = adata[sampled_cells, sampled_genes].copy()\n",
    "adata_subset.write('resources_test/grn_benchmark/evaluation_data/nakatake_bulk.h5ad')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
